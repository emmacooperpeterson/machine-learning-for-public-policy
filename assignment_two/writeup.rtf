{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf820
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 My code for this assignment can perform the following tasks:\
\
- Read data into a dataframe\
- Fill in missing values\
- Covert column names to snake_case\
- Review the dataframe\'92s shape, column headers, and data types\
- View how many values are missing in each column\
- Obtain a count for each unique value in a column\
- View descriptive statistics (min, max, mean, etc.) for each column\
- Create a histogram, box plot, or pie chart for a given column\
- Discretize a continuous variable into categorical bins based on percentiles\
- Create dummy variables from a categorical variable\
- Run a logistic regression\
- View some evaluation metrics based on the regression, including precision, recall, accuracy, etc.\
\
\
On the provided data:\
\
- Created an overview and saw that there were some values missing in two columns \'96 filled those cells with the mean of their respective columns\
- Reviewed the distributions for some of the columns by creating charts and frequency counts (these functions could be performed on any column)\
- Binned monthly incomes into ranges based on percentile (ten percentiles)\
- Binned age into ranges based on percentiles (three percentiles)\
- Created dummy variables on age, describing whether or not a given observation falls into a given age bin\
- Ran a logistic regression with the output variable \'91SeriousDlqin2yrs\'92, using each column individually as a feature \'96 combinations of columns could also be used as features, but columns were just used individually for the purpose of this assignment\
\
\
Results (using training data = testing data):\
\
- 93% of observations were \'91no\'92 for SeriousDlqin2yrs, so simply predicting \'91no\'92 on everything would result in 93% accuracy.\
- Each feature variable produced relatively high precision (~.87) and recall (~.93)\
- Confusion matrices showed that most feature variables led to zero false positives and zero false negatives.\
- NumberOfTimes30-59DaysPastDueNotWorse, NumberOfTimes90DaysLate, and NumberOfTimes60-89DaysPastDueNotWorse achieved slightly higher precision (.91), as well as an accuracy score just slightly better than random (.9333 > .9331)\
- Using training sets / test sets would yield more accurate/meaningful results.\
- Combining features may also help to produce higher precision/accuracy.\
\
\
\
\
\
}